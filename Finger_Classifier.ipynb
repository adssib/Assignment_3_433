{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c171dd6",
   "metadata": {},
   "source": [
    "# 1. Preprocessing Dataset\n",
    "\n",
    "Overfitting: Monitor train vs validation gap; apply dropout/augmentation\n",
    "\n",
    "Computational efficiency: Start with smaller images/models, scale up if needed\n",
    "\n",
    "Reproducibility: Set random seeds for consistent results\n",
    "\n",
    "\n",
    "Must Have \n",
    "\n",
    "✅ Architecture details (layers, filters, activations)\n",
    "\n",
    "✅ Final hyperparameters (LR, epochs, batch size, optimizer, loss)\n",
    "\n",
    "✅ Training/Validation accuracy plot\n",
    "\n",
    "✅ Test accuracy\n",
    "\n",
    "✅ Precision, Recall, F1 per class\n",
    "\n",
    "✅ Confusion matrix\n",
    "\n",
    "✅ Brief analysis/observations\n",
    "\n",
    "\n",
    "### **B. Data Transformations**:\n",
    "- **For training**: Augmentation (random rotations, flips, color jitter) + normalize\n",
    "- **For validation/test**: Just resize + normalize (no augmentation)\n",
    "\n",
    "### **C. DataLoaders**:\n",
    "- Train loader with shuffling\n",
    "- Test/validation loader without shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6df50d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def set_seed(seed=433):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(433)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set matplotlib to display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# For better plot quality\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2480e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train directory exists: True\n",
      "Test directory exists: True\n"
     ]
    }
   ],
   "source": [
    "fingers_train_dir = 'data/fingers/train'\n",
    "fingers_test_dir = 'data/fingers/test'\n",
    "\n",
    "print(f\"Train directory exists: {os.path.exists(fingers_train_dir)}\")\n",
    "print(f\"Test directory exists: {os.path.exists(fingers_test_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59be86d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training images: 18000\n",
      "Total test images: 3600\n",
      "\n",
      "==================================================\n",
      "First 15 training filenames:\n",
      "==================================================\n",
      " 1. df16050b-6ce3-4a09-8406-f314c9303090_3R.png\n",
      " 2. ae08efc1-b453-49f3-80f9-d4b3dacd83e0_5R.png\n",
      " 3. 401d2efa-d6fd-4116-878c-38f4806d8b97_0L.png\n",
      " 4. 69a97cd5-f3b5-4246-90cc-14173995ffea_2R.png\n",
      " 5. f510e888-1eec-43c4-9e16-a591e68e47f9_4R.png\n",
      " 6. c98821ba-4fcc-440b-b74f-87d34b526efa_4L.png\n",
      " 7. 76de966c-103d-4879-85b4-6bd1c85f968c_1R.png\n",
      " 8. fa50c7b1-4bf8-412d-a979-b6cc75029b65_5L.png\n",
      " 9. 608e588f-ca5a-4e1a-82ba-bed244920c16_3R.png\n",
      "10. 8fdd76c0-5615-4e87-b88c-496fcc20840d_1R.png\n",
      "11. 38f0a970-957f-42b4-b5d9-7f1e1d8a3fa5_0R.png\n",
      "12. 9640b47c-c37f-45ea-925c-21f380662adb_1R.png\n",
      "13. a64a2165-6442-4430-b4db-8b58f384e471_4R.png\n",
      "14. 6615d35f-14c1-4909-b069-6d6f936a7ce2_4R.png\n",
      "15. 572ff08a-f791-4d7b-8a97-1a169b98b2e7_5R.png\n"
     ]
    }
   ],
   "source": [
    "fingers_train_files = os.listdir(fingers_train_dir)\n",
    "fingers_test_files = os.listdir(fingers_test_dir)\n",
    "\n",
    "print(f\"Total training images: {len(fingers_train_files)}\")\n",
    "print(f\"Total test images: {len(fingers_test_files)}\")\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"First 15 training filenames:\")\n",
    "print(f\"{'='*50}\")\n",
    "for i, filename in enumerate(fingers_train_files[:15]):\n",
    "    print(f\"{i+1:2d}. {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d00a0c",
   "metadata": {},
   "source": [
    "### We are going to ommit the 'R' or 'L' completly since the assignment only tlks about categorising 3,4,5 and didn't mention right or left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55d2a0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename                                 -> Label\n",
      "==================================================\n",
      "df16050b-6ce3-4a09-8406-f314c9303090_3R.png -> 3\n",
      "ae08efc1-b453-49f3-80f9-d4b3dacd83e0_5R.png -> 5\n",
      "401d2efa-d6fd-4116-878c-38f4806d8b97_0L.png -> 0\n",
      "69a97cd5-f3b5-4246-90cc-14173995ffea_2R.png -> 2\n",
      "f510e888-1eec-43c4-9e16-a591e68e47f9_4R.png -> 4\n"
     ]
    }
   ],
   "source": [
    "def extract_label(filename):\n",
    "    try:\n",
    "        name = filename.split('.')[0]\n",
    "        label_part = name.split('_')[-1]\n",
    "        digit = label_part[0]\n",
    "        return int(digit)\n",
    "    except:\n",
    "        print(f\"Warning: Could not parse {filename}\")\n",
    "        return None\n",
    "    \n",
    "print(f\"{'Filename':<40} -> Label\")\n",
    "print(\"=\"*50)\n",
    "for filename in fingers_train_files[:5]:\n",
    "    label = extract_label(filename)\n",
    "    print(f\"{filename:<40} -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161fb957",
   "metadata": {},
   "source": [
    "### Checking for Class imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5151ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Class Distribution in Training Set:\n",
      "==================================================\n",
      "Class 0: 3000 images\n",
      "Class 1: 3000 images\n",
      "Class 2: 3000 images\n",
      "Class 3: 3000 images\n",
      "Class 4: 3000 images\n",
      "Class 5: 3000 images\n",
      "==================================================\n",
      "Total: 18000 images\n"
     ]
    }
   ],
   "source": [
    "train_labels = []\n",
    "for f in fingers_train_files:\n",
    "    if f.endswith(('.jpg', '.png', '.jpeg')):\n",
    "        label = extract_label(f)\n",
    "        if label is not None:\n",
    "            train_labels.append(label)\n",
    "\n",
    "label_counts = Counter(train_labels)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Class Distribution in Training Set:\")\n",
    "print(f\"{'='*50}\")\n",
    "for class_num in sorted(label_counts.keys()):\n",
    "    print(f\"Class {class_num}: {label_counts[class_num]:4d} images\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total: {sum(label_counts.values())} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "837266f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking image dimensions from random samples...\n",
      "\n",
      "Filename                                      Width    Height   Mode    \n",
      "===========================================================================\n",
      "584e7e1a-8a38-48b8-8d00-a67734d57d3d_3L.png   128      128      L       \n",
      "b1da3784-b409-4f5a-ae32-5e3a63ba52d9_3R.png   128      128      L       \n",
      "20e6622e-c11c-4204-a1e9-140ed481d2b8_4R.png   128      128      L       \n",
      "4f3a51f6-543b-4712-ad8a-aa5026a7d8bf_5R.png   128      128      L       \n",
      "4e2b0ae3-e55a-4168-9c03-ef09bc3402dc_0R.png   128      128      L       \n",
      "65de1b21-a7be-4a2b-b31d-e7eeeca6de77_4R.png   128      128      L       \n",
      "f11052dd-efe8-441a-8d81-2a81e64ab281_0L.png   128      128      L       \n",
      "5951b638-5654-45ce-a10a-c07c0d46f142_1L.png   128      128      L       \n",
      "369b4cce-0e4a-4b06-975c-5ef1919fcc70_3R.png   128      128      L       \n",
      "d592eb1d-0195-4354-b07b-4290d921cb6f_2L.png   128      128      L       \n",
      "===========================================================================\n",
      "\n",
      "Image Statistics:\n",
      "  Most common width:  128\n",
      "  Most common height: 128\n",
      "  Width range: 128 - 128\n",
      "  Height range: 128 - 128\n",
      "\n",
      "All images have the SAME dimensions: 128x128\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking image dimensions from random samples...\\n\")\n",
    "print(f\"{'Filename':<45} {'Width':<8} {'Height':<8} {'Mode':<8}\")\n",
    "print(\"=\"*75)\n",
    "\n",
    "sample_files = random.sample(fingers_train_files, min(10, len(fingers_train_files)))\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "for filename in sample_files:\n",
    "    if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
    "        img_path = os.path.join(fingers_train_dir, filename)\n",
    "        img = Image.open(img_path)\n",
    "        width, height = img.size\n",
    "        widths.append(width)\n",
    "        heights.append(height)\n",
    "        print(f\"{filename:<45} {width:<8} {height:<8} {img.mode:<8}\")\n",
    "\n",
    "print(\"=\"*75)\n",
    "print(f\"\\nImage Statistics:\")\n",
    "print(f\"  Most common width:  {max(set(widths), key=widths.count)}\")\n",
    "print(f\"  Most common height: {max(set(heights), key=heights.count)}\")\n",
    "print(f\"  Width range: {min(widths)} - {max(widths)}\")\n",
    "print(f\"  Height range: {min(heights)} - {max(heights)}\")\n",
    "\n",
    "if len(set(widths)) == 1 and len(set(heights)) == 1:\n",
    "    print(f\"\\nAll images have the SAME dimensions: {widths[0]}x{heights[0]}\")\n",
    "else:\n",
    "    print(f\"\\nImages have DIFFERENT dimensions - we'll need to resize them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c1fd401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FingerCountDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.image_files = [f for f in os.listdir(data_dir) \n",
    "                           if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        \n",
    "        self.labels = [self._extract_label(f) for f in self.image_files]\n",
    "        \n",
    "        self.classes = sorted(list(set(self.labels)))\n",
    "        self.num_classes = len(self.classes)\n",
    "        \n",
    "        print(f\" Loaded {len(self.image_files)} images from {data_dir}\")\n",
    "        print(f\"   Number of classes: {self.num_classes} (classes: {self.classes})\")\n",
    "        \n",
    "    def _extract_label(self, filename):\n",
    "        name = filename.split('.')[0]\n",
    "        label_part = name.split('_')[-1]\n",
    "        digit = int(label_part[0])\n",
    "        return digit\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')  # NEED TO Convert grayscale to RGB\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "675c435a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations defined\n",
      "   Training: Augmentation + Normalization\n",
      "   Test: Normalization only\n"
     ]
    }
   ],
   "source": [
    "# Normalization values (ImageNet standard)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Training transformations (WITH augmentation) - NO RESIZE NEEDED\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Test transformations (NO augmentation) - NO RESIZE NEEDED\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "print(\"Transformations defined\")\n",
    "print(\"   Training: Augmentation + Normalization\")\n",
    "print(\"   Test: Normalization only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7a1e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 18000 images from data/fingers/train\n",
      "   Number of classes: 6 (classes: [0, 1, 2, 3, 4, 5])\n",
      " Loaded 3600 images from data/fingers/test\n",
      "   Number of classes: 6 (classes: [0, 1, 2, 3, 4, 5])\n",
      "\n",
      "============================================================\n",
      "Dataset Summary:\n",
      "============================================================\n",
      "Training samples:   18000\n",
      "Test samples:       3600\n",
      "Training batches:   282\n",
      "Test batches:       57\n",
      "Batch size:         64\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = FingerCountDataset(\n",
    "    data_dir=fingers_train_dir,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "test_dataset = FingerCountDataset(\n",
    "    data_dir=fingers_test_dir,\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "# Hyperparameters, this is current when developping the model we will change it and try diffrent stuff\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Dataset Summary:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training samples:   {len(train_dataset)}\")\n",
    "print(f\"Test samples:       {len(test_dataset)}\")\n",
    "print(f\"Training batches:   {len(train_loader)}\")\n",
    "print(f\"Test batches:       {len(test_loader)}\")\n",
    "print(f\"Batch size:         {BATCH_SIZE}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8d9d0",
   "metadata": {},
   "source": [
    "### Now after all of this let's verfiy the Preprocessing Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75ebce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " Pipeline Verification:\n",
      "============================================================\n",
      "Batch shape:      torch.Size([64, 3, 128, 128])\n",
      "Labels shape:     torch.Size([64])\n",
      "Image data type:  torch.float32\n",
      "Image range:      [-2.118, 2.640]\n",
      "Sample labels:    [2, 4, 5, 1, 4, 5, 5, 2, 2, 5]\n",
      "============================================================\n",
      "\n",
      " PREPROCESSING COMPLETE AND VERIFIED!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\" Pipeline Verification:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Batch shape:      {images.shape}\")  # Should be [64, 3, 128, 128]\n",
    "print(f\"Labels shape:     {labels.shape}\")  # Should be [64]\n",
    "print(f\"Image data type:  {images.dtype}\")\n",
    "print(f\"Image range:      [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(f\"Sample labels:    {labels[:10].tolist()}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\n PREPROCESSING COMPLETE AND VERIFIED!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785c8f9",
   "metadata": {},
   "source": [
    "## 2. CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cedcb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Three CNN architectures defined:\n",
      "   1. SimpleCNN - 3 conv blocks, baseline\n",
      "   2. DeepCNN - 4 conv blocks, deeper network\n",
      "   3. WideCNN - 3 conv blocks, more filters\n"
     ]
    }
   ],
   "source": [
    "# Architecture 1: Simple CNN (Baseline)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=6, dropout_rate=0.5):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Conv Block 1\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # 128 -> 64\n",
    "        \n",
    "        # Conv Block 2\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # 64 -> 32\n",
    "        \n",
    "        # Conv Block 3\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # 32 -> 16\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 256)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv blocks\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Architecture 2: Deeper CNN\n",
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self, num_classes=6, dropout_rate=0.5):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        \n",
    "        # Conv Block 1\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Conv Block 2\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # 128 -> 64\n",
    "        \n",
    "        # Conv Block 3\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # 64 -> 32\n",
    "        \n",
    "        # Conv Block 4\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)  # 32 -> 16\n",
    "        \n",
    "        # FC layers\n",
    "        self.fc1 = nn.Linear(256 * 16 * 16, 512)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Architecture 3: Wide CNN (More filters per layer)\n",
    "class WideCNN(nn.Module):\n",
    "    def __init__(self, num_classes=6, dropout_rate=0.5):\n",
    "        super(WideCNN, self).__init__()\n",
    "        \n",
    "        # Conv Block 1\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # 128 -> 64\n",
    "        \n",
    "        # Conv Block 2\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # 64 -> 32\n",
    "        \n",
    "        # Conv Block 3\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # 32 -> 16\n",
    "        \n",
    "        # FC layers\n",
    "        self.fc1 = nn.Linear(256 * 16 * 16, 512)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "print(\" Three CNN architectures defined:\")\n",
    "print(\"   1. SimpleCNN - 3 conv blocks, baseline\")\n",
    "print(\"   2. DeepCNN - 4 conv blocks, deeper network\")\n",
    "print(\"   3. WideCNN - 3 conv blocks, more filters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb63a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model1 = SimpleCNN().to(device)\n",
    "model2 = DeepCNN().to(device)\n",
    "model3 = WideCNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d51800",
   "metadata": {},
   "source": [
    "## 3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ce3f9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing forward pass with sample batch:\n",
      "Input shape: torch.Size([64, 3, 128, 128])\n",
      "\n",
      " All models working correctly!\n",
      "SimpleCNN output shape: torch.Size([64, 6])\n",
      "DeepCNN output shape:   torch.Size([64, 6])\n",
      "WideCNN output shape:   torch.Size([64, 6])\n",
      "\n",
      " Architectures ready for training!\n"
     ]
    }
   ],
   "source": [
    "sample_images, sample_labels = next(iter(train_loader))\n",
    "sample_images = sample_images.to(device)\n",
    "\n",
    "print(f\"Testing forward pass with sample batch:\")\n",
    "print(f\"Input shape: {sample_images.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output1 = model1(sample_images)\n",
    "    output2 = model2(sample_images)\n",
    "    output3 = model3(sample_images)\n",
    "\n",
    "print(f\"\\n All models working correctly!\")\n",
    "print(f\"SimpleCNN output shape: {output1.shape}\")\n",
    "print(f\"DeepCNN output shape:   {output2.shape}\")\n",
    "print(f\"WideCNN output shape:   {output3.shape}\")\n",
    "print(f\"\\n Architectures ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e440d643",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acf7d73",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
